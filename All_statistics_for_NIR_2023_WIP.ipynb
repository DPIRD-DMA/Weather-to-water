{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a0a1f6c2",
      "metadata": {
        "id": "a0a1f6c2"
      },
      "source": [
        "CODE TRANSLATION R_TO_PYTHON SECTION 1-10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be55cb81",
      "metadata": {
        "id": "be55cb81"
      },
      "source": [
        "Index:\n",
        "1) ORGANISE THE DATA (RUN IN HPC)\n",
        "2) COMPARE EMISSION FACTORS ACROSS TIERS\n",
        "3) ESTIMATE THE NUMBER OF FARM DAMS IN 2015\n",
        "4) GET THE STATISTICS ON SURFACE AREA FOR THE LAST FINANCIAL YEAR AT EACH MONTH FOR EACH STATE AND TERRITORY\n",
        "5) GET HISTORICAL TRENDS\n",
        "6) CALCULATE STATISTICS AND RATES\n",
        "7) PLOTS AND SAVE DATA\n",
        "8) PLOT THE FULL DATASET\n",
        "9) ALL STATISTICS\n",
        "10) SOURCES OF UNCERTAINTY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c205a3d",
      "metadata": {
        "id": "2c205a3d"
      },
      "source": [
        "SECTION 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0d171a65",
      "metadata": {
        "id": "0d171a65"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "from pandas.api.types import CategoricalDtype\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "from shapely import wkt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b9dd0433",
      "metadata": {},
      "outputs": [],
      "source": [
        "# a) Load maps\n",
        "AusDams = gpd.read_file(\"Required data/AusDams.geojson\")\n",
        "AUSstates = gpd.read_file(\"Required data/States Map_cropped.shp\")\n",
        "AusClimate = rasterio.open(\"Required data/clim-zones.tif\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "963ddfb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "Data = pd.read_csv(\"working/Dam forecast preds v6.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e046c95f",
      "metadata": {},
      "outputs": [],
      "source": [
        "Data['geometry'] = Data['geometry'].apply(wkt.loads)\n",
        "Data = gpd.GeoDataFrame(Data, geometry='geometry')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6b6a55e5",
      "metadata": {
        "id": "6b6a55e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unexpected exception formatting exception. Falling back to standard exception\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/nick/mambaforge/envs/gis/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipykernel_2184271/831037287.py\", line 9, in <module>\n",
            "    Data['ClimateCode'] = Data.apply(lambda row: AusClimate.read(1, window=Data.bounds, masked=True)[row.y, row.x], axis=1)\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nick/mambaforge/envs/gis/lib/python3.11/site-packages/geopandas/geodataframe.py\", line 1581, in apply\n",
            "    result = super().apply(\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/home/nick/mambaforge/envs/gis/lib/python3.11/site-packages/pandas/core/frame.py\", line 9423, in apply\n",
            "    return op.apply().__finalize__(self, method=\"apply\")\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/nick/mambaforge/envs/gis/lib/python3.11/site-packages/pandas/core/apply.py\", line 678, in apply\n",
            "    return self.apply_standard()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nick/mambaforge/envs/gis/lib/python3.11/site-packages/pandas/core/apply.py\", line 798, in apply_standard\n",
            "    results, res_index = self.apply_series_generator()\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nick/mambaforge/envs/gis/lib/python3.11/site-packages/pandas/core/apply.py\", line 814, in apply_series_generator\n",
            "    results[i] = self.f(v)\n",
            "                 ^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_2184271/831037287.py\", line 9, in <lambda>\n",
            "    Data['ClimateCode'] = Data.apply(lambda row: AusClimate.read(1, window=Data.bounds, masked=True)[row.y, row.x], axis=1)\n",
            "                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"rasterio/_io.pyx\", line 585, in rasterio._io.DatasetReaderBase.read\n",
            "  File \"/home/nick/mambaforge/envs/gis/lib/python3.11/site-packages/pandas/core/generic.py\", line 1466, in __nonzero__\n",
            "    raise ValueError(\n",
            "ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/nick/mambaforge/envs/gis/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nick/mambaforge/envs/gis/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1428, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nick/mambaforge/envs/gis/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1319, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nick/mambaforge/envs/gis/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1172, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nick/mambaforge/envs/gis/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1062, in format_exception_as_a_whole\n",
            "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nick/mambaforge/envs/gis/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1154, in get_records\n",
            "    FrameInfo(\n",
            "  File \"/home/nick/mambaforge/envs/gis/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 780, in __init__\n",
            "    ix = inspect.getsourcelines(frame)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nick/mambaforge/envs/gis/lib/python3.11/inspect.py\", line 1244, in getsourcelines\n",
            "    lines, lnum = findsource(object)\n",
            "                  ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nick/mambaforge/envs/gis/lib/python3.11/inspect.py\", line 1073, in findsource\n",
            "    raise OSError('source code not available')\n",
            "OSError: source code not available\n"
          ]
        }
      ],
      "source": [
        "# Nick's data\n",
        "# Assuming the file \"full_preds_v5.csv\" has been loaded into a DataFrame named \"Data\"\n",
        "\n",
        "# Add the locations\n",
        "Data['x'] = Data['geometry'].apply(lambda geom: geom.x)\n",
        "Data['y'] = Data['geometry'].apply(lambda geom: geom.y)\n",
        "\n",
        "# Get the climate for each farm dam\n",
        "Data['ClimateCode'] = Data.apply(lambda row: AusClimate.read(1, window=Data.bounds, masked=True)[row.y, row.x], axis=1)\n",
        "\n",
        "# Add the state where the point comes from\n",
        "e = AusDams.total_bounds\n",
        "r = rasterio.Raster(e, height=1000, width=1000)\n",
        "AUSstates_f = gpd.overlay(AUSstates, gpd.GeoDataFrame({'geometry': r.features()}, geometry='geometry'))\n",
        "Data = gpd.sjoin(Data, AUSstates_f, how='left', op='within')\n",
        "\n",
        "# Clean up and reformat Nick's data\n",
        "Data2 = Data[(Data['area'] > 100) & (Data['area'] < 80000)]\n",
        "Data2 = Data2[Data2.filter(like='X').gt(0).all(axis=1)]\n",
        "\n",
        "# Pivot data\n",
        "Data3 = pd.melt(Data2, id_vars=['State.ID', 'x', 'y', 'area', 'area_2', 'NAME', 'geometry'],\n",
        "                value_vars=Data2.filter(like='X').columns,\n",
        "                var_name='Date', value_name='Area')\n",
        "Data3['Date'] = pd.to_datetime(Data3['Date'].str.replace('X', '').str.replace('.00.00.00', ''), format='%Y.%m.%d')\n",
        "\n",
        "# Create FinYear\n",
        "Data3['FinYear'] = (Data3['Date'] + pd.DateOffset(months=6)).dt.year.astype(str)\n",
        "\n",
        "# d) Tier 2 approach\n",
        "Tier2EF_dict = pd.read_excel(\"Required data/EF and climate zone dictionary.xlsx\", dtype=str)\n",
        "Data4 = Data3.merge(Tier2EF_dict, on='ClimateCode', how='left')\n",
        "\n",
        "# e) Tier 3 approach\n",
        "Temp2 = pd.read_pickle(\"Required data/Temp.pkl\")\n",
        "Temp2 = pd.melt(Temp2, id_vars=['X', 'Y'], value_vars=Temp2.filter(like='X').columns,\n",
        "                var_name='Date', value_name='Temp')\n",
        "Temp2['Date'] = pd.to_datetime(Temp2['Date'].str.replace('X', '').str.replace('_tav', '01'), format='%Y%m%d')\n",
        "Data5 = pd.merge(Data4, Temp2, on=['x', 'y', 'Date'], how='left')\n",
        "\n",
        "# f) Run the analyses\n",
        "Boltzmann_const_evK = 8.617333262145 * 1e-5\n",
        "FarmDams_CH4kgyearha_at15C = 365.25 * 10000 * (10 ** TotMet_15C_lm.params[0]) / 1e+6\n",
        "FarmDams_CH4kgyearha_at15C_LCI = 365.25 * 10000 * (10 ** TotMet_15C_lm.conf_int()[0, 0]) / 1e+6\n",
        "FarmDams_CH4kgyearha_at15C_UCI = 365.25 * 10000 * (10 ** TotMet_15C_lm.conf_int()[1, 0]) / 1e+6\n",
        "Emissions_LCI = 100 * (FarmDams_CH4kgyearha_at15C - FarmDams_CH4kgyearha_at15C_LCI) / FarmDams_CH4kgyearha_at15C\n",
        "Emissions_UCI = 100 * (FarmDams_CH4kgyearha_at15C_UCI - FarmDams_CH4kgyearha_at15C) / FarmDams_CH4kgyearha_at15C\n",
        "Emissions_CV = np.mean(np.concatenate([Emissions_LCI, Emissions_UCI]))\n",
        "\n",
        "# Correct for temperature\n",
        "Data5['Boltzmann_15C'] = (1 / (Boltzmann_const_evK * (273.15 + 15))) - \\\n",
        "                         (1 / (Boltzmann_const_evK * (273.15 + Data5['Temp'])))\n",
        "\n",
        "# Load the temp sensitivity parameter\n",
        "Boltzmann_lm = pd.read_pickle(\"Required data/Boltzmann.pkl\")\n",
        "Data5['EF_T3'] = np.exp(Boltzmann_lm.params[1] * Data5['Boltzmann_15C'] +\n",
        "                        np.log(FarmDams_CH4kgyearha_at15C))\n",
        "Data5['EF_T3_LCI'] = np.exp(Boltzmann_lm.conf_int()[0, 1] * Data5['Boltzmann_15C'] +\n",
        "                            np.log(FarmDams_CH4kgyearha_at15C_LCI))\n",
        "Data5['EF_T3_UCI'] = np.exp(Boltzmann_lm.conf_int()[1, 1] * Data5['Boltzmann_15C'] +\n",
        "                            np.log(FarmDams_CH4kgyearha_at15C_UCI))\n",
        "\n",
        "# Calculate EF2 and 3 for each dam (kg CH4 per year per dam)\n",
        "Data5['TotE_T3_kg_year'] = Data5['Area'] * Data5['EF_T3'] / 10000\n",
        "Data5['TotE_T3_kg_year_LCI'] = (Data5['Area'] - 0.45 * Data5['Area']) * Data5['EF_T3_LCI'] / 10000\n",
        "Data5['TotE_T3_kg_year_UCI'] = (Data5['Area'] + 0.45 * Data5['Area']) * Data5['EF_T3_UCI'] / 10000\n",
        "Data5['TotE_T2_kg_year'] = Data5['Area'] * Data5['EF_T2'] / 10000\n",
        "\n",
        "# g) Calculate the relative percentage capacity from Nick's data\n",
        "Data2_T3 = Data5.copy()\n",
        "Data2_T3['RelSurfaceArea'] = Data2_T3['Area'] / Data2_T3['area']\n",
        "\n",
        "# Save smaller version of the file\n",
        "Data2_T3_small = Data2_T3[['x', 'y', 'Date', 'ClimateCode', 'NAME', 'Area', 'FinYear', 'EF_T2',\n",
        "                           'TotE_T2_kg_year', 'EF_T3', 'TotE_T3_kg_year', 'TotE_T3_kg_year_LCI',\n",
        "                           'TotE_T3_kg_year_UCI']]\n",
        "Data2_T3_small.to_pickle(\"Results/Data/Data2_T3_small_HPC.pkl\")\n",
        "Data2_T3.to_pickle(\"Results/Data/Data2_T3_HPC.pkl\")\n",
        "\n",
        "# Create a summary for the average water capacity in farm dams for each year\n",
        "RelCapacity = Data2_T3.groupby(['Date', 'NAME', 'FinYear'])['RelSurfaceArea'].mean().reset_index()\n",
        "RelCapacity.to_pickle(\"Results/Data/RelCapacity.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05752aec",
      "metadata": {
        "id": "05752aec"
      },
      "source": [
        "SECTION 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8c6f192",
      "metadata": {
        "id": "a8c6f192"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b630f44",
      "metadata": {
        "id": "8b630f44"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import curve_fit\n",
        "from scipy.stats import nbinom\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "# 2) COMPARE EMISSION FACTORS ACROSS TIERS\n",
        "if False:\n",
        "    # Create the dataset\n",
        "    TierComp = pd.DataFrame({'Temp': range(0, 51)})\n",
        "\n",
        "    # Get temp-sensitivity\n",
        "    Boltzmann_const_evK = 8.617333262145e-5\n",
        "\n",
        "    # Get Boltzmann factor across temps\n",
        "    TierComp['Boltzmann_15C'] = (1 / (Boltzmann_const_evK * (273.15 + 15))) - \\\n",
        "                                (1 / (Boltzmann_const_evK * (273.15 + TierComp['Temp'])))\n",
        "\n",
        "    # Load the average farm dam emissions at 15C in mg.m2.day\n",
        "    TotMet_15C = pd.read_csv(\"Required data/TotMet_15C.csv\")\n",
        "    FarmDams_CH4kgyearha_at15C = 365.25 * 10000 * (10 ** TotMet_15C['TotMet']) / 1e+6\n",
        "    FarmDams_CH4kgyearha_at15C_LCI = 365.25 * 10000 * (10 ** TotMet_15C['TotMet_lower']) / 1e+6\n",
        "    FarmDams_CH4kgyearha_at15C_UCI = 365.25 * 10000 * (10 ** TotMet_15C['TotMet_upper']) / 1e+6\n",
        "\n",
        "    # Get emissions T3 (kt.year.ha)\n",
        "    TierComp['EF_T3'] = np.exp(Boltzmann_lq[1] * TierComp['Boltzmann_15C'] +\n",
        "                               np.log(FarmDams_CH4kgyearha_at15C)) * 1000 / 1000000\n",
        "    TierComp['EF_T3_LCI'] = np.exp(Boltzmann_lq[1] * TierComp['Boltzmann_15C'] +\n",
        "                                   np.log(FarmDams_CH4kgyearha_at15C_LCI)) * 1000 / 1000000\n",
        "    TierComp['EF_T3_UCI'] = np.exp(Boltzmann_lq[1] * TierComp['Boltzmann_15C'] +\n",
        "                                   np.log(FarmDams_CH4kgyearha_at15C_UCI)) * 1000 / 1000000\n",
        "\n",
        "    # Get emissions T1 (kt.year.ha)\n",
        "    TierComp['EF_T1'] = 183 * 1000 / 1000000\n",
        "    TierComp['EF_T1_LCI'] = 118 * 1000 / 1000000\n",
        "    TierComp['EF_T1_UCI'] = 228 * 1000 / 1000000\n",
        "\n",
        "    # Get emissions T2, climate-specific EF following Tertius directions (Kg CH4 ha-1 year-1)\n",
        "    Tier2EF_dict = pd.read_excel(\"Required data/EF and climate zone dictionary.xlsx\", sheet_name=\"Restructure2\")\n",
        "    Tier2EF_dict['EF_T2.kt'] = Tier2EF_dict['EF_T2'] * 1000 / 1000000\n",
        "    Tier2EF_dict['Climate'] = Tier2EF_dict['Climate'].astype('category')\n",
        "\n",
        "    TierComp2 = TierComp.merge(Tier2EF_dict, on='Temp', how='outer')\n",
        "\n",
        "    # Extract the CI from the relationship\n",
        "    T2_raw = pd.read_excel(\"Required data/Farmdam EF values_COPY.xlsx\", sheet_name=\"Working data set_R\")\n",
        "    T2_raw['EF_kg.CH4.ha.y'] = np.log10(T2_raw['EF_kg.CH4.ha.y'])\n",
        "    T2_raw['Temp'] = T2_raw['Temp'] + 273.15\n",
        "\n",
        "    def func(x, a, b):\n",
        "        return a * x ** b\n",
        "\n",
        "    popt, _ = curve_fit(func, T2_raw['Temp'], T2_raw['EF_kg.CH4.ha.y'])\n",
        "    T2_raw['pred'] = func(T2_raw['Temp'], *popt)\n",
        "\n",
        "    prdc = np.random.normal(loc=T2_raw['pred'], scale=0.1, size=(1000, len(T2_raw)))\n",
        "\n",
        "    # Calculate prediction intervals\n",
        "    prdc_CI = pd.DataFrame({\n",
        "        'Temp': T2_raw['Temp'],\n",
        "        'LCI.kt': 10 ** (np.percentile(prdc, 5, axis=1)) * 1000 / 1000000,\n",
        "        'UCI.kt': 10 ** (np.percentile(prdc, 95, axis=1)) * 1000 / 1000000,\n",
        "        'LCI': 10 ** (np.percentile(prdc, 5, axis=1)),\n",
        "        'UCI': 10 ** (np.percentile(prdc, 95, axis=1))\n",
        "    })\n",
        "\n",
        "    prdc_CI['Pred.kt'] = 10 ** (np.round(prdc_CI['Temp'], 2)) * 1000 / 1000000\n",
        "    prdc_CI['Pred'] = 10 ** (np.round(prdc_CI['Temp'], 2))\n",
        "\n",
        "    # Check the fit\n",
        "    plt.figure()\n",
        "    plt.plot(T2_raw['Temp'], prdc.T, color='gray', alpha=0.5)\n",
        "    plt.plot(T2_raw['Temp'], T2_raw['EF_kg.CH4.ha.y'], 'o')\n",
        "    plt.plot(T2_raw['Temp'], T2_raw['pred'])\n",
        "    plt.fill_between(prdc_CI['Temp'], prdc_CI['LCI'], prdc_CI['UCI'], color='blue', alpha=0.1)\n",
        "    plt.show()\n",
        "\n",
        "    # Log-10 scale\n",
        "    plt.figure()\n",
        "    plt.plot(T2_raw['Temp'], prdc.T, color='gray', alpha=0.5)\n",
        "    plt.plot(T2_raw['Temp'], T2_raw['EF_kg.CH4.ha.y'], 'o')\n",
        "    plt.plot(T2_raw['Temp'], np.log10(prdc_CI['Pred']))\n",
        "    plt.fill_between(prdc_CI['Temp'], np.log10(prdc_CI['LCI']), np.log10(prdc_CI['UCI']), color='blue', alpha=0.1)\n",
        "    plt.show()\n",
        "\n",
        "    # Aritmnetic scale\n",
        "    plt.figure()\n",
        "    plt.plot(T2_raw['Temp'], 10 ** prdc.T, color='gray', alpha=0.5)\n",
        "    plt.plot(T2_raw['Temp'], T2_raw['EF_kg.CH4.ha.y'], 'o')\n",
        "    plt.plot(T2_raw['Temp'], prdc_CI['Pred'])\n",
        "    plt.fill_between(prdc_CI['Temp'], prdc_CI['LCI'], prdc_CI['UCI'], color='blue', alpha=0.1)\n",
        "    plt.show()\n",
        "\n",
        "    # Check for patterns\n",
        "    print(prdc_CI['LCI'] / prdc_CI['Pred'])\n",
        "    print(prdc_CI['UCI'] / prdc_CI['Pred'])\n",
        "    print(prdc_CI['Pred'] / prdc_CI['UCI'])\n",
        "\n",
        "    # Averages\n",
        "    print(np.mean(prdc_CI['LCI'] / prdc_CI['Pred']))\n",
        "    print(np.mean(prdc_CI['UCI'] / prdc_CI['Pred']))\n",
        "\n",
        "    # Solution:\n",
        "    # For the LCI, I will multiply the predicted log-10 value by 0.8432157\n",
        "    # For the UCI, I will multiply the predicted log-10 value by 1.100405\n",
        "\n",
        "    TierComp2['EF_T2.kt_LCI'] = 10 ** (np.log10(TierComp2['EF_T2.kt']) * 0.8604673)\n",
        "    TierComp2['EF_T2.kt_UCI'] = 10 ** (np.log10(TierComp2['EF_T2.kt']) * 1.1085)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure()\n",
        "    plt.plot(TierComp2.loc[TierComp2['Temp'] < 41, 'Temp'], TierComp2.loc[TierComp2['Temp'] < 41, 'EF_T3_LCI'],\n",
        "             alpha=0.1, label='Tier 3 LCI')\n",
        "    plt.plot(TierComp2.loc[TierComp2['Temp'] < 41, 'Temp'], TierComp2.loc[TierComp2['Temp'] < 41, 'EF_T3'],\n",
        "             label='Tier 3', linewidth=1.5)\n",
        "    plt.plot(TierComp2.loc[TierComp2['Temp'] < 41, 'Temp'], TierComp2.loc[TierComp2['Temp'] < 41, 'EF_T1_LCI'],\n",
        "             alpha=0.1, label='Tier 1 LCI')\n",
        "    plt.plot(TierComp2.loc[TierComp2['Temp'] < 41, 'Temp'], TierComp2.loc[TierComp2['Temp'] < 41, 'EF_T1'],\n",
        "             label='Tier 1', linestyle='dashed', linewidth=1.5)\n",
        "\n",
        "    unique_climates = TierComp2['Climate'].unique()\n",
        "    color_map = plt.cm.get_cmap('viridis', len(unique_climates))\n",
        "\n",
        "    for i, climate in enumerate(unique_climates):\n",
        "        temp_range = TierComp2.loc[TierComp2['Temp'] < 41, 'Temp']\n",
        "        ef_lci = TierComp2.loc[TierComp2['Temp'] < 41].loc[TierComp2['Climate'] == climate, 'EF_T2.kt_LCI']\n",
        "        ef_uci = TierComp2.loc[TierComp2['Temp'] < 41].loc[TierComp2['Climate'] == climate, 'EF_T2.kt_UCI']\n",
        "        plt.fill_between(temp_range, ef_lci, ef_uci, color=color_map(i), alpha=0.1)\n",
        "        plt.plot(temp_range, TierComp2.loc[TierComp2['Temp'] < 41].loc[TierComp2['Climate'] == climate, 'EF_T2.kt'],\n",
        "                 color=color_map(i), linewidth=1.5, label=f'Tier 2 {climate}')\n",
        "\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xlim(0, 41)\n",
        "    plt.xlabel('Mean annual temperature (°C)')\n",
        "    plt.ylabel('Methane emission factors (EF; t CH₄ ha⁻¹ year⁻¹)')\n",
        "    plt.legend()\n",
        "    plt.title('Comparison of Emission Factors Across Tiers')\n",
        "    plt.annotate('Temp-dependent EF', xy=(36, 0.84), xytext=(36, 0.84), size=8)\n",
        "    plt.annotate('Temp-independent EF', xy=(36, 0.15), xytext=(36, 0.15), size=8)\n",
        "    plt.show()\n",
        "\n",
        "    # Save the plot as PDF\n",
        "    plt.savefig('Results/Plot/Emission_factors.pdf', format='pdf', dpi=300)\n",
        "\n",
        "    # Save the plot as JPEG\n",
        "    plt.savefig('Results/Plot/Emission_factors.jpeg', format='jpeg', dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73ee80a8",
      "metadata": {
        "id": "73ee80a8"
      },
      "source": [
        "SECTION 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7b5952c",
      "metadata": {
        "id": "b7b5952c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fc83be8",
      "metadata": {
        "id": "9fc83be8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask.dataframe as dd\n",
        "from dask.distributed import Client\n",
        "\n",
        "# Create a Dask client for parallel processing\n",
        "client = Client(n_workers=10)\n",
        "\n",
        "# Load Data2.T3.small.HPC.RData\n",
        "data2_t3_small = dd.read_parquet(\"Results/Data/Data2.T3.small.HPC.parquet\")\n",
        "data2_t3_small = data2_t3_small.compute()\n",
        "\n",
        "# Get the final statistics from Malerba et al (2021)\n",
        "counts_df = pd.read_csv(\"Required data/FinalOutcome.s.csv\")\n",
        "counts_df = counts_df[['State.f', 'Count.50', 'Count.2.5', 'Count.97.5']].rename(columns={'State.f': 'State'})\n",
        "\n",
        "# Calculate the adjusted coefficients\n",
        "counts_df['AdjCoef'] = counts_df['Count.50'] / data2_t3_small.groupby('NAME')['index'].nunique()\n",
        "counts_df['AdjCoef.LCI'] = counts_df['Count.2.5'] / data2_t3_small.groupby('NAME')['index'].nunique()\n",
        "counts_df['AdjCoef.UCI'] = counts_df['Count.97.5'] / data2_t3_small.groupby('NAME')['index'].nunique()\n",
        "\n",
        "# Save the results\n",
        "counts_df.to_csv(\"Results/Data/AllCounts.csv\", index=False)\n",
        "counts_df.to_pickle(\"Results/Data/AllCounts.pkl\")\n",
        "\n",
        "# AllCounts: Number of farm dams in 2015 from Malerba et al 2021 Remote Sensing\n",
        "import dask.dataframe as dd\n",
        "\n",
        "AllCounts = dd.read_csv(\"Results/Data/AllCounts.csv\").compute()\n",
        "\n",
        "# Close the Dask client\n",
        "client.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31cb88b8",
      "metadata": {
        "id": "31cb88b8"
      },
      "source": [
        "SECTION 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea0c9d0b",
      "metadata": {
        "id": "ea0c9d0b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db3eed1e",
      "metadata": {
        "id": "db3eed1e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from dplython import (DplyFrame, X, select, group_by, summarize, arrange, left_join, merge, mutate, filter)\n",
        "\n",
        "# Load data (change file paths accordingly)\n",
        "Data2_T3_Small_HPC = pd.read_pickle(\"Results/Data/Data2.T3.Small.HPC.pkl\")\n",
        "AllCounts = pd.read_pickle(\"Results/Data/AllCounts.pkl\")\n",
        "\n",
        "# Define a function to calculate adjusted values\n",
        "def calculate_adj_values(df):\n",
        "    df['SumArea.kha.Adj'] = df['SumArea.kha'] * df['AdjCoef']\n",
        "    df['SumArea.kha.Adj.LCI'] = df['SumArea.kha_LCI'] * df['AdjCoef.LCI']\n",
        "    df['SumArea.kha.Adj.UCI'] = df['SumArea.kha_UCI'] * df['AdjCoef.UCI']\n",
        "    df['TotE_T3.kt.year.Adj'] = df['TotE_T3.kt.year'] * df['AdjCoef']\n",
        "    df['TotE_T3.kt.year.Adj.LCI'] = df['TotE_T3.kt.year_LCI'] * df['AdjCoef.LCI']\n",
        "    df['TotE_T3.kt.year.Adj.UCI'] = df['TotE_T3.kt.year_UCI'] * df['AdjCoef.UCI']\n",
        "    df['TotE_T2.kt.year.Adj'] = df['TotE_T2.kt.year'] * df['AdjCoef']\n",
        "    return df\n",
        "\n",
        "# Group by State, FinYear and calculate summary statistics\n",
        "Data2_T3_kha = (\n",
        "    Data2_T3_Small_HPC\n",
        "    # Filter data here if needed\n",
        "    .groupby(['NAME', 'FinYear'])\n",
        "    .agg(\n",
        "        SumArea_kha=pd.NamedAgg(column='Area', aggfunc='sum') * 1e-7,\n",
        "        SumArea_kha_LCI=pd.NamedAgg(column='Area', aggfunc=lambda x: sum(x) * 1e-7 - 0.45 * sum(x) * 1e-7),\n",
        "        SumArea_kha_UCI=pd.NamedAgg(column='Area', aggfunc=lambda x: sum(x) * 1e-7 + 0.45 * sum(x) * 1e-7),\n",
        "        SumTotE_T3_kt_year=pd.NamedAgg(column='TotE_T3.kg.year', aggfunc='sum') / 1000000,\n",
        "        SumTotE_T3_kt_LCI_year=pd.NamedAgg(column='TotE_T3.kg.year_LCI', aggfunc='sum') / 1000000,\n",
        "        SumTotE_T3_kt_UCI_year=pd.NamedAgg(column='TotE_T3.kg.year_UCI', aggfunc='sum') / 1000000,\n",
        "        SumTotE_T2_kt_year=pd.NamedAgg(column='TotE_T2.kg.year', aggfunc='sum') / 1000000\n",
        "    )\n",
        "    .reset_index()\n",
        "    .rename(columns={'NAME': 'State'})\n",
        "    .pipe(calculate_adj_values)\n",
        "    .merge(AllCounts, on='State')\n",
        ")\n",
        "\n",
        "# Extract years with full data\n",
        "TableYear = (Data2_T3_kha['FinYear'].value_counts() == 96).reset_index()\n",
        "FullYear = TableYear.loc[TableYear['FinYear'], 'index']\n",
        "\n",
        "# Keep only years with all observations\n",
        "Data2_T3_kha = Data2_T3_kha[Data2_T3_kha['FinYear'].isin(FullYear)]\n",
        "\n",
        "# Save data (change file path accordingly)\n",
        "Data2_T3_kha.to_pickle(\"Results/Data/Data2.T3.kha.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a949479",
      "metadata": {
        "id": "4a949479"
      },
      "outputs": [],
      "source": [
        "# Load necessary data (change file paths accordingly)\n",
        "Data2_T3_kha = pd.read_pickle(\"Results/Data/Data2.T3.kha.pkl\")\n",
        "Data3_T3_kha = Data2_T3_kha.groupby(['State', 'FinYear']).agg(\n",
        "    SumArea_kha=pd.NamedAgg(column='SumArea.kha', aggfunc='mean'),\n",
        "    SumArea_kha_LCI=pd.NamedAgg(column='SumArea.kha_LCI', aggfunc='mean'),\n",
        "    SumArea_kha_UCI=pd.NamedAgg(column='SumArea.kha_UCI', aggfunc='mean'),\n",
        "    TotE_T3_kt_year=pd.NamedAgg(column='SumTotE_T3.kt.year', aggfunc='mean'),\n",
        "    TotE_T3_kt_LCI_year=pd.NamedAgg(column='SumTotE_T3.kt.year.LCI', aggfunc='mean'),\n",
        "    TotE_T3_kt_UCI_year=pd.NamedAgg(column='SumTotE_T3.kt.year.UCI', aggfunc='mean'),\n",
        "    TotE_T2_kt_year=pd.NamedAgg(column='SumTotE_T2.kt.year', aggfunc='mean')\n",
        ").reset_index()\n",
        "\n",
        "Data3_T3_kha = Data3_T3_kha.merge(AllCounts, on='State')\n",
        "\n",
        "# Calculate adjusted values\n",
        "Data3_T3_kha = calculate_adj_values(Data3_T3_kha)\n",
        "\n",
        "# Calculate NT relative density\n",
        "NT_reldensity = AllCounts.loc[AllCounts['State'] == \"Northern Territory\", 'Count.50'].values[0] / AllCounts['Count.50'].sum()\n",
        "\n",
        "# Calculate NT statistics\n",
        "Data3_kha_NT = (\n",
        "    Data3_T3_kha[Data3_T3_kha['State'] != \"Northern Territory\"]\n",
        "    .groupby('FinYear')\n",
        "    .agg(\n",
        "        SumArea_kha_Adj=pd.NamedAgg(column='SumArea.kha.Adj', aggfunc='sum') * NT_reldensity,\n",
        "        SumArea_kha_Adj_LCI=pd.NamedAgg(column='SumArea.kha.Adj.LCI', aggfunc='sum') * NT_reldensity,\n",
        "        SumArea_kha_Adj_UCI=pd.NamedAgg(column='SumArea.kha.Adj.UCI', aggfunc='sum') * NT_reldensity,\n",
        "        TotE_T2_kt_year_Adj=pd.NamedAgg(column='TotE_T2.kt.year.Adj', aggfunc='sum') * NT_reldensity,\n",
        "        TotE_T3_kt_year_Adj=pd.NamedAgg(column='TotE_T3.kt.year.Adj', aggfunc='sum') * NT_reldensity,\n",
        "        TotE_T3_kt_year_Adj_LCI=pd.NamedAgg(column='TotE_T3.kt.year.Adj.LCI', aggfunc='sum') * NT_reldensity,\n",
        "        TotE_T3_kt_year_Adj_UCI=pd.NamedAgg(column='TotE_T3.kt.year.Adj.UCI', aggfunc='sum') * NT_reldensity\n",
        "    )\n",
        "    .reset_index()\n",
        "    .assign(State=\"Northern Territory\")\n",
        ")\n",
        "\n",
        "# Combine NT statistics with other states\n",
        "Data3_kha_all = pd.concat([Data3_kha_NT, Data3_T3_kha[Data3_T3_kha['State'] != \"Northern Territory\"]])\n",
        "\n",
        "# Calculate Australia-wide statistics\n",
        "Data4_kha = (\n",
        "    Data3_kha_all.groupby('FinYear')\n",
        "    .agg(\n",
        "        SumArea_kha_Adj=pd.NamedAgg(column='SumArea.kha.Adj', aggfunc='sum'),\n",
        "        SumArea_kha_Adj_LCI=pd.NamedAgg(column='SumArea.kha.Adj.LCI', aggfunc='sum'),\n",
        "        SumArea_kha_Adj_UCI=pd.NamedAgg(column='SumArea.kha.Adj.UCI', aggfunc='sum'),\n",
        "        TotE_T2_kt_year_Adj=pd.NamedAgg(column='TotE_T2.kt.year.Adj', aggfunc='sum'),\n",
        "        TotE_T3_kt_year_Adj=pd.NamedAgg(column='TotE_T3.kt.year.Adj', aggfunc='sum'),\n",
        "        TotE_T3_kt_year_Adj_LCI=pd.NamedAgg(column='TotE_T3.kt.year.Adj.LCI', aggfunc='sum'),\n",
        "        TotE_T3_kt_year_Adj_UCI=pd.NamedAgg(column='TotE_T3.kt.year.Adj.UCI', aggfunc='sum')\n",
        "    )\n",
        "    .reset_index()\n",
        "    .assign(State=\"Australia\")\n",
        ")\n",
        "\n",
        "# Combine Australia-wide statistics with other states\n",
        "Data5_kha = pd.concat([Data4_kha, Data3_kha_all])\n",
        "\n",
        "# Save the final datasets (change file paths accordingly)\n",
        "Data3_T3_kha.to_pickle(\"Results/Data/Data3.T3.kha.pkl\")\n",
        "Data3_kha_NT.to_pickle(\"Results/Data/Data3.kha.NT.pkl\")\n",
        "Data3_kha_all.to_pickle(\"Results/Data/Data3.kha.all.pkl\")\n",
        "Data4_kha.to_pickle(\"Results/Data/Data4.kha.pkl\")\n",
        "Data5_kha.to_pickle(\"Results/Data/Data5.kha.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e8fe13",
      "metadata": {
        "id": "48e8fe13"
      },
      "outputs": [],
      "source": [
        "# Load necessary data (change file paths accordingly)\n",
        "Data3_T3_kha = pd.read_pickle(\"Results/Data/Data3.T3.kha.pkl\")\n",
        "#Data3_kha_NT = pd.read_pickle(\"Results/Data/Data3.kha.NT.pkl\")\n",
        "#Data3_kha_all = pd.read_pickle(\"Results/Data/Data3.kha.all.pkl\")\n",
        "Data4_kha = pd.read_pickle(\"Results/Data/Data4.kha.pkl\")\n",
        "Data5_kha = pd.read_pickle(\"Results/Data/Data5.kha.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60cfe321",
      "metadata": {
        "id": "60cfe321"
      },
      "source": [
        "SECTION 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f539df8",
      "metadata": {
        "id": "7f539df8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "115db89b",
      "metadata": {
        "id": "115db89b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load required data\n",
        "# ====================\n",
        "# MyECDF.single\n",
        "my_ecdf_single = pd.read_csv(\"Required data/MyECDF.single.csv\")\n",
        "my_ecdf_single.rename(columns={\"X2\": \"State\"}, inplace=True)\n",
        "\n",
        "# Create MyECDF.allyears\n",
        "unique_states = my_ecdf_single[\"State\"].unique()\n",
        "unique_years = my_ecdf_single[\"YearBuilt\"].unique()\n",
        "my_ecdf_allyears = pd.DataFrame({'State': np.repeat(unique_states, len(unique_years)),\n",
        "                                 'YearBuilt': np.tile(unique_years, len(unique_states))})\n",
        "\n",
        "# Merge with MyECDF.single to fill missing values\n",
        "my_ecdf_single = pd.merge(my_ecdf_allyears, my_ecdf_single, on=[\"State\", \"YearBuilt\"], how=\"left\")\n",
        "\n",
        "# Replace missing values in Australian Capital Territory\n",
        "act_indices = (my_ecdf_single[\"State\"] == \"Australian Capital Territory\")\n",
        "act_ecd = my_ecdf_single.loc[act_indices, \"ecd\"]\n",
        "act_ecd = act_ecd.fillna(act_ecd.rolling(window=2).mean())  # Replace with a simple moving average\n",
        "my_ecdf_single.loc[act_indices, \"ecd\"] = act_ecd\n",
        "\n",
        "# Correct missing values in Australian Capital Territory for certain years\n",
        "act_missing_years = [1995, 1996, 1997]\n",
        "my_ecdf_single.loc[act_indices & my_ecdf_single[\"YearBuilt\"].isin(act_missing_years), \"ecd\"] = \\\n",
        "    0.6393443 + np.diff([0.6393443, 0.6721311]) / 4 * np.array([1, 2, 3])\n",
        "\n",
        "# Calculate ecd for Northern Territory based on overall mean for Australia\n",
        "my_ecdf_single2 = my_ecdf_single.groupby(\"YearBuilt\")[\"ecd\"].mean().reset_index()\n",
        "my_ecdf_single2[\"State\"] = \"Northern Territory\"\n",
        "\n",
        "# Merge with other States\n",
        "my_ecdf_single3 = pd.concat([my_ecdf_single2, my_ecdf_single])\n",
        "\n",
        "# Calculate the projected rate of increase after 2005 for each State\n",
        "my_ecdf_single3_recent = my_ecdf_single3[my_ecdf_single3[\"YearBuilt\"] > 2005]\n",
        "relative_growth_lm = sm.OLS(my_ecdf_single3_recent[\"ecd\"], sm.add_constant(my_ecdf_single3_recent[[\"YearBuilt\", \"State\"]])).fit()\n",
        "predict_data = pd.DataFrame({'State': unique_states,\n",
        "                             'YearBuilt': np.arange(2005, 2021)})\n",
        "predict_data[\"Pred\"] = relative_growth_lm.predict(sm.add_constant(predict_data[[\"YearBuilt\", \"State\"]]))\n",
        "\n",
        "# Visualize historical trends\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.set(style=\"whitegrid\")\n",
        "g = sns.FacetGrid(data=my_ecdf_single3, col=\"State\", col_wrap=3, height=4)\n",
        "g.map(plt.step, \"YearBuilt\", \"ecd\", color='b', where=\"mid\")\n",
        "g.map(plt.plot, \"YearBuilt\", \"Pred\", color='r')\n",
        "g.set_axis_labels(\"Year\", \"Historical increase in dam density\")\n",
        "g.set_titles(col_template=\"{col_name}\")\n",
        "g.add_legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"Results/Plot/Predict.Plot.pdf\")\n",
        "\n",
        "# Generate predictions for each State\n",
        "final_predictions = pd.DataFrame({'State': unique_states,\n",
        "                                 'YearBuilt': np.arange(2010, 2021)})\n",
        "final_predictions[\"ecd.pred\"] = relative_growth_lm.predict(sm.add_constant(final_predictions[[\"YearBuilt\", \"State\"]]))\n",
        "\n",
        "# Combine observations and predictions\n",
        "hist_data = pd.merge(my_ecdf_single3, final_predictions, on=[\"State\", \"YearBuilt\"], how=\"left\")\n",
        "hist_data[\"ecd.comb\"] = np.where(hist_data[\"ecd\"].isnull(), hist_data[\"ecd.pred\"], hist_data[\"ecd\"])\n",
        "hist_data[\"ecd.adj\"] = hist_data[\"ecd.comb\"] / hist_data.groupby(\"State\")[\"ecd.comb\"].transform(\"max\")\n",
        "\n",
        "# Rename columns and convert FinYear to numeric\n",
        "hist_data.rename(columns={\"YearBuilt\": \"FinYear\"}, inplace=True)\n",
        "hist_data[\"FinYear\"] = hist_data[\"FinYear\"].astype(int)\n",
        "\n",
        "# Load other data\n",
        "data5_kha_hist = pd.read_csv(\"Results/Data/Data5.kha.hist.csv\")\n",
        "all_counts_aus = pd.read_csv(\"Results/Data/AllCounts.aus.csv\")\n",
        "\n",
        "# Merge data and perform necessary calculations\n",
        "data6_kha_hist = pd.merge(data5_kha_hist, hist_data, on=[\"State\", \"FinYear\"], how=\"left\")\n",
        "data6_kha_hist = data6_kha_hist.merge(all_counts_aus[[\"State\", \"Count.50\"]], on=\"State\", how=\"left\")\n",
        "data6_kha_hist[\"SumCount.Adj.ecd\"] = data6_kha_hist[\"Count.50\"] * data6_kha_hist[\"ecd.adj\"]\n",
        "data6_kha_hist[\"SumCount.Adj.LCI.ecd\"] = data6_kha_hist[\"Count.50\"] * 0.025 * data6_kha_hist[\"ecd.adj\"]\n",
        "data6_kha_hist[\"SumCount.Adj.UCI.ecd\"] = data6_kha_hist[\"Count.50\"] * 0.975 * data6_kha_hist[\"ecd.adj\"]\n",
        "\n",
        "# Whole Australia statistics by summing across the States and Territories\n",
        "data6_kha_hist_aus = data6_kha_hist.groupby(\"FinYear\").agg({\"SumArea.kha.Adj.ecd\": \"sum\",\n",
        "                                                           \"SumArea.kha.Adj.LCI.ecd\": \"sum\",\n",
        "                                                           \"SumArea.kha.Adj.UCI.ecd\": \"sum\",\n",
        "                                                           \"TotE_T3.kt.year.Adj.ecd\": \"sum\",\n",
        "                                                           \"TotE_T3.kt.year.Adj.LCI.ecd\": \"sum\",\n",
        "                                                           \"TotE_T3.kt.year.Adj.UCI.ecd\": \"sum\",\n",
        "                                                           \"SumCount.Adj.ecd\": \"sum\",\n",
        "                                                           \"SumCount.Adj.LCI.ecd\": \"sum\",\n",
        "                                                           \"SumCount.Adj.UCI.ecd\": \"sum\"}).reset_index()\n",
        "data6_kha_hist_aus[\"State\"] = \"Australia\"\n",
        "\n",
        "# Save data\n",
        "data6_kha_hist.to_csv(\"Results/Data/Data6.kha.hist.csv\", index=False)\n",
        "data6_kha_hist_aus.to_csv(\"Results/Data/Data6.kha.hist.Aus.csv\", index=False)\n",
        "\n",
        "\n",
        "# Load data at the end of the script\n",
        "data6_kha_hist = pd.read_csv(\"Results/Data/Data6.kha.hist.csv\")\n",
        "data6_kha_hist_aus = pd.read_csv(\"Results/Data/Data6.kha.hist.Aus.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "022a0133",
      "metadata": {
        "id": "022a0133"
      },
      "source": [
        "SECTION 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00408e6d",
      "metadata": {
        "id": "00408e6d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4bbd458",
      "metadata": {
        "id": "f4bbd458"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 6) PLOTTING PREDICTED INCREASES IN DAM DENSITY\n",
        "# =============\n",
        "\n",
        "# NOTE: SumCount.Adj.ecd are the predicted values based on observations and linear regression,\n",
        "#       SumCount.ecd are only based on observations\n",
        "\n",
        "# Calculate the projected rate of increase in total numbers after 2005 for each State\n",
        "subset_data = Data6_kha_hist[Data6_kha_hist['FinYear'] > 2005]\n",
        "\n",
        "X = sm.add_constant(subset_data[['FinYear', 'State']])\n",
        "y = subset_data['SumCount.ecd']\n",
        "abs_growth_model = sm.OLS(y, X).fit()\n",
        "\n",
        "unique_states = Data6_kha_hist['State'].unique()\n",
        "predict_data_abs = pd.DataFrame({'State': np.repeat(unique_states, 16), 'FinYear': np.tile(np.arange(2005, 2021), 8)})\n",
        "predict_data_abs = sm.add_constant(predict_data_abs)\n",
        "predict_data_abs['Pred'] = abs_growth_model.predict(predict_data_abs)\n",
        "\n",
        "# Plot total farm dam numbers and projected increases\n",
        "plt.figure(figsize=(9.5, 10))\n",
        "sns.set_style(\"whitegrid\")\n",
        "for state in unique_states:\n",
        "    state_data = Data6_kha_hist[Data6_kha_hist['State'] == state]\n",
        "    plt.step(state_data['FinYear'], state_data['SumCount.ecd'] / 1000, where='mid', label=state, linewidth=2)\n",
        "    predict_state = predict_data_abs[predict_data_abs['State'] == state]\n",
        "    plt.plot(predict_state['FinYear'], predict_state['Pred'] / 1000, label=f'Predicted {state}', linewidth=2, linestyle='dashed')\n",
        "\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Historical increase in dam numbers (thousands)')\n",
        "plt.title('Predicted Increases in Farm Dam Density')\n",
        "plt.legend()\n",
        "plt.xlim(1988, 2020)\n",
        "plt.grid(False)\n",
        "plt.savefig('Results/Plot/PredictAbs.Plot.png')\n",
        "plt.close()\n",
        "\n",
        "# RESULTS: Calculate relative increase from 1988 to 2020\n",
        "state_summary = []\n",
        "for state in unique_states:\n",
        "    counts_1988 = Data6_kha_hist[(Data6_kha_hist['State'] == state) & (Data6_kha_hist['FinYear'] == 1988)]['SumCount.Adj.ecd'].values[0]\n",
        "    counts_2020 = Data6_kha_hist[(Data6_kha_hist['State'] == state) & (Data6_kha_hist['FinYear'] == 2020)]['SumCount.Adj.ecd'].values[0]\n",
        "    rel_count_1988_to_2020 = (counts_2020 - counts_1988) / counts_1988\n",
        "    sa_1988 = Data6_kha_hist[(Data6_kha_hist['State'] == state) & (Data6_kha_hist['FinYear'] == 1988)]['SumArea.kha.Adj.ecd'].values[0]\n",
        "    sa_2020 = Data6_kha_hist[(Data6_kha_hist['State'] == state) & (Data6_kha_hist['FinYear'] == 2020)]['SumArea.kha.Adj.ecd'].values[0]\n",
        "    rel_sa_1988_to_2020 = (sa_2020 - sa_1988) / sa_1988\n",
        "    abs_count_1988_to_2020 = counts_2020 - counts_1988\n",
        "    abs_count_LCI_1988_to_2020 = Data6_kha_hist[(Data6_kha_hist['State'] == state) & (Data6_kha_hist['FinYear'] == 2020)]['SumCount.Adj.LCI.ecd'].values[0] - \\\n",
        "                                  Data6_kha_hist[(Data6_kha_hist['State'] == state) & (Data6_kha_hist['FinYear'] == 1988)]['SumCount.Adj.LCI.ecd'].values[0]\n",
        "    abs_count_UCI_1988_to_2020 = Data6_kha_hist[(Data6_kha_hist['State'] == state) & (Data6_kha_hist['FinYear'] == 2020)]['SumCount.Adj.UCI.ecd'].values[0] - \\\n",
        "                                  Data6_kha_hist[(Data6_kha_hist['State'] == state) & (Data6_kha_hist['FinYear'] == 1988)]['SumCount.Adj.UCI.ecd'].values[0]\n",
        "    abs_sa_1988_to_2020 = sa_2020 - sa_1988\n",
        "    abs_sa_LCI_1988_to_2020 = Data6_kha_hist[(Data6_kha_hist['State'] == state) & (Data6_kha_hist['FinYear'] == 2020)]['SumArea.kha.Adj.LCI.ecd'].values[0] - \\\n",
        "                              Data6_kha_hist[(Data6_kha_hist['State'] == state) & (Data6_kha_hist['FinYear'] == 1988)]['SumArea.kha.Adj.LCI.ecd'].values[0]\n",
        "    abs_sa_UCI_1988_to_2020 = Data6_kha_hist[(Data6_kha_hist['State'] == state) & (Data6_kha_hist['FinYear'] == 2020)]['SumArea.kha.Adj.UCI.ecd'].values[0] - \\\n",
        "                              Data6_kha_hist[(Data6_kha_hist['State'] == state) & (Data6_kha_hist['FinYear'] == 1988)]['SumArea.kha.Adj.UCI.ecd'].values[0]\n",
        "    state_summary.append([state, counts_1988, counts_2020, rel_count_1988_to_2020, sa_1988, sa_2020, rel_sa_1988_to_2020,\n",
        "                          abs_count_1988_to_2020, abs_count_LCI_1988_to_2020, abs_count_UCI_1988_to_2020,\n",
        "                          abs_sa_1988_to_2020, abs_sa_LCI_1988_to_2020, abs_sa_UCI_1988_to_2020])\n",
        "\n",
        "state_summary_columns = ['State', 'Counts_1988', 'Counts_2020', 'Rel_Count_1988to2020', 'SA_1988', 'SA_2020', 'Rel_SA_1988to2020',\n",
        "                         'Abs_Count_1988to2020', 'Abs_Count_LCI_1988to2020', 'Abs_Count_UCI_1988to2020',\n",
        "                         'Abs_SA_1988to2020', 'Abs_SA_LCI_1988to2020', 'Abs_SA_UCI_1988to2020']\n",
        "\n",
        "in_cr_df = pd.DataFrame(state_summary, columns=state_summary_columns)\n",
        "in_cr_df.sort_values(by='Abs_Count_1988to2020', ascending=False, inplace=True)\n",
        "\n",
        "in_cr_df.to_csv(\"Results/Data/Incr1988to2020.csv\", index=False)\n",
        "in_cr_df.to_pickle(\"Results/Data/Incr1988to2020.pkl\")\n",
        "\n",
        "# Incr1988to2020: Predicted increase from 1988 to 2020\n",
        "in_cr_df = pd.read_pickle(\"Results/Data/Incr1988to2020.pkl\")\n",
        "\n",
        "#Please note that this translation assumes you have already loaded the Data6_kha_hist DataFrame earlier in your code.\n",
        "#Adjust the code accordingly based on your specific setup."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2520d8ee",
      "metadata": {
        "id": "2520d8ee"
      },
      "source": [
        "SECTION 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2860f7b",
      "metadata": {
        "id": "a2860f7b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fb6455e",
      "metadata": {
        "id": "1fb6455e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mizani.breaks import date_breaks\n",
        "from mizani.formatters import date_format\n",
        "from plotnine import ggplot, aes, geom_area, geom_line, labs, theme_bw, \\\n",
        "    facet_wrap, guides, scale_x_continuous, scale_y_continuous, sec_axis\n",
        "from plotnine.arrange import plot_grid\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Load Rainfall Anomalies data\n",
        "rain_anomalies = pd.read_table(\"Required data/Rainfall anomalies.txt\", names=[\"FinYear\", \"Anomaly\"])\n",
        "rain_anomalies[\"FinYear\"] = rain_anomalies[\"FinYear\"].str[:4].astype(int)\n",
        "rain_anomalies_sub = rain_anomalies[rain_anomalies[\"FinYear\"].isin(Data6_kha_hist[\"FinYear\"].unique())]\n",
        "\n",
        "# Load RelCapacity\n",
        "rel_capacity = pd.read_pickle(\"Results/Data/RelCapacity.pkl\")\n",
        "\n",
        "# Create RelCapacityBOM.aus dataset\n",
        "rel_capacity_bom_aus = (\n",
        "    rel_capacity\n",
        "    .assign(FinYear=lambda x: pd.to_datetime(x[\"Date\"]).dt.strftime(\"%Y-%m-%d\"))\n",
        "    .groupby(\"FinYear\")[\"RelSurfaceArea\"]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .merge(rain_anomalies_sub, on=\"FinYear\", how=\"inner\")\n",
        ")\n",
        "\n",
        "rel_capacity_bom_aus.to_pickle(\"Results/Data/RelCapacityBOM_aus.pkl\")\n",
        "\n",
        "# Plot Predict.Plot.area.tot2.T3\n",
        "plot_area_tot2_T3 = (\n",
        "    ggplot(Data6_kha_hist.query(\"State != 'Australia'\"), aes(x=\"FinYear\", y=\"SumArea.kha.Adj.ecd\"))\n",
        "    + geom_area(aes(fill=\"State\"))\n",
        "    + labs(fill=\"\", x=\"Financial Year\")\n",
        "    + theme_bw()\n",
        "    + theme(\n",
        "        panel_grid_major=None,\n",
        "        panel_grid_minor=None,\n",
        "        legend_position=(0.1, 0.85)\n",
        "    )\n",
        "    + scale_x_continuous(breaks=date_breaks(\"5 years\"), labels=date_format(\"%Y\"))\n",
        "    + ggsave(\"Results/Plot/Predict.Plot.area2.T3.pdf\", width=12.3, height=7)\n",
        ")\n",
        "\n",
        "plot_area_tot2_T3.save(\"Results/Plot/Predict.Plot.area2.T3.pdf\", width=12.3, height=7)\n",
        "\n",
        "plot_area_tot2_T3.show()\n",
        "\n",
        "# Plot Predict.Plot.area.tot2.T3\n",
        "plot_area_tot2_T3 = (\n",
        "    ggplot(Data6_kha_hist.query(\"State != 'Australia'\"), aes(x=\"FinYear\", y=\"SumArea.kha.Adj.ecd\"))\n",
        "    + geom_area(aes(fill=\"State\"))\n",
        "    + geom_line(\n",
        "        data=rain_anomalies_sub.query(\"FinYear > 1987\"),\n",
        "        aes(x=\"FinYear\", y=\"Anomaly\"),\n",
        "        color=\"red\",\n",
        "        size=2\n",
        "    )\n",
        "    + labs(fill=\"\", x=\"Financial Year\")\n",
        "    + theme_bw()\n",
        "    + scale_x_continuous(breaks=date_breaks(\"5 years\"), labels=date_format(\"%Y\"))\n",
        "    + scale_y_continuous(\n",
        "        name=\"Historical increase in dam water surface area (area; kha)\",\n",
        "        sec_axis=sec_axis(trans=\"identity\", name=\"Rainfall anomalies (line)\")\n",
        "    )\n",
        "    + theme(\n",
        "        panel_grid_major=None,\n",
        "        panel_grid_minor=None,\n",
        "        legend_position=(0.15, 0.85)\n",
        "    )\n",
        ")\n",
        "\n",
        "plot_area_tot2_T3.save(\"Results/Plot/Predict.Plot.area3.T3.pdf\", width=11.5, height=6.5)\n",
        "plot_area_tot2_T3.show()\n",
        "\n",
        "# Calculate Pearson's correlation\n",
        "correlation = rel_capacity_bom_aus.query(\"FinYear > 1988\")[\"RelSurfaceArea\"].corr(\n",
        "    rel_capacity_bom_aus.query(\"FinYear > 1988\")[\"Anomaly\"]\n",
        ")\n",
        "print(\"Pearson's correlation:\", correlation)\n",
        "\n",
        "# Lineplot of average relative capacity and BOM rainfall anomalies\n",
        "plot_area_tot2_T4_a = (\n",
        "    ggplot(rel_capacity_bom_aus.query(\"FinYear > 1988\"), aes(x=\"FinYear\", y=\"RelSurfaceArea\"))\n",
        "    + geom_line(color=\"black\", size=1)\n",
        "    + geom_line(\n",
        "        data=rain_anomalies_sub.query(\"FinYear > 1988\"),\n",
        "        aes(x=\"FinYear\", y=(rain_anomalies_sub[\"Anomaly\"] / 4000) + 0.45),\n",
        "        linetype=\"dashed\"\n",
        "    )\n",
        "    + labs(x=\"Financial Year\")\n",
        "    + theme_bw()\n",
        "    + scale_x_continuous(breaks=date_breaks(\"5 years\"), labels=date_format(\"%Y\"))\n",
        "    + scale_y_continuous(\n",
        "        name=\"Mean predicted water surface capacity (solid; %)\",\n",
        "        sec_axis=sec_axis(\n",
        "            trans=lambda x: (x - 0.45) * 4000,\n",
        "            name=\"Rainfall anomalies (dashed)\"\n",
        "        )\n",
        "    )\n",
        "    + theme(\n",
        "        panel_grid_major=None,\n",
        "        panel_grid_minor=None,\n",
        "        legend_position=(0.15, 0.85)\n",
        "    )\n",
        ")\n",
        "\n",
        "plot_area_tot2_T4_a.save(\"Results/Plot/Predict.Plot.area.tot2.T4.pdf\", width=11.5, height=6.5)\n",
        "plot_area_tot2_T4_a.show()\n",
        "\n",
        "# Scatterplot of average relative capacity and BOM rainfall anomalies\n",
        "plot_area_tot2_T4_b = (\n",
        "    ggplot(rel_capacity_bom_aus.query(\"FinYear > 1988\"), aes(x=\"Anomaly\", y=\"RelSurfaceArea\"))\n",
        "    + geom_point(size=3)\n",
        "    + geom_smooth(method=\"lm\", se=False, color=\"black\", linetype=\"dashed\")\n",
        "    + labs(x=\"Rainfall anomalies\", y=\"Mean predicted water surface capacity (%)\")\n",
        "    + theme_bw()\n",
        "    + theme(panel_grid_major=None, panel_grid_minor=None)\n",
        ")\n",
        "\n",
        "plot_area_tot2_T4_b.save(\"Results/Plot/Predict.Plot.area.tot2.T4.b.pdf\", width=11.5, height=6.5)\n",
        "plot_area_tot2_T4_b.show()\n",
        "\n",
        "# Combine plots\n",
        "plot_area_tot2_ab_T4 = plot_grid(plot_area_tot2_T4_a, plot_area_tot2_T4_b, ncol=1, labels=[\"A\", \"B\"])\n",
        "plot_area_tot2_ab_T4.save(\"Results/Plot/Predict.Plot.area.tot2.ab.T4.pdf\", width=6.72, height=10.45)\n",
        "plot_area_tot2_ab_T4.save(\"Results/Plot/Predict.Plot.area.tot2.ab.T4.jpeg\", width=6.72, height=10.45)\n",
        "\n",
        "# Load RelCapacityBOM.aus\n",
        "rel_capacity_bom_aus = pd.read_pickle(\"Results/Data/RelCapacityBOM_aus.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1841f521",
      "metadata": {
        "id": "1841f521"
      },
      "source": [
        "SECTION 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b73972cd",
      "metadata": {
        "id": "b73972cd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c71dce6",
      "metadata": {
        "id": "6c71dce6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from plotnine import ggplot, aes, geom_line, geom_ribbon, facet_wrap, labs, theme_bw, theme, scale_x_continuous\n",
        "from plotnine import save_as_pdf_pages, save_as_jpeg_pages\n",
        "from plotnine import save_plot\n",
        "from mizani.breaks import date_breaks\n",
        "from plotnine import ggtitle, guide_legend, guides\n",
        "\n",
        "# Note: Due to the structure of the code, the content inside the \"if(FALSE)\" block is ignored in the Python version.\n",
        "\n",
        "# Assuming Data6.kha.hist.Aus, Data6.kha.hist, and Data5.kha are available as DataFrames\n",
        "\n",
        "# TotEmissions.Aus\n",
        "TotEmissions_Aus = (\n",
        "    ggplot(Data6.kha.hist.Aus, aes(x = pd.to_numeric(FinYear) + 1, y = TotE_T3_kt_year_Adj_ecd))\n",
        "    + geom_line(show_legend = False)\n",
        "    + geom_ribbon(aes(ymin = TotE_T3_kt_year_Adj_LCI_ecd, ymax = TotE_T3_kt_year_Adj_UCI_ecd), alpha = 0.4)\n",
        "    + theme_bw()\n",
        "    + labs(y = \"Total methane emissions (CH$_4$ kt year$^{-1}$)\", x = \"Year\")\n",
        "    + theme(panel_grid_major = element_blank(), panel_grid_minor = element_blank(), legend_position = \"top\")\n",
        "    + scale_x_continuous(limits = (1988, 2020))\n",
        ")\n",
        "\n",
        "save_plot(TotEmissions_Aus, filename = \"Results/Plot/TotEmissions_Aus.pdf\", width = 7, height = 7)\n",
        "save_plot(TotEmissions_Aus, filename = \"Results/Plot/TotEmissions_Aus.jpeg\", width = 7, height = 7)\n",
        "\n",
        "\n",
        "# TotEmissions (Divided by States)\n",
        "TotEmissions = (\n",
        "    ggplot(Data6.kha.hist, aes(x = pd.to_numeric(FinYear) + 1, y = TotE_T3_kt_year_Adj_ecd))\n",
        "    + geom_line(show_legend = False)\n",
        "    + geom_ribbon(aes(ymin = TotE_T3_kt_year_Adj_LCI_ecd, ymax = TotE_T3_kt_year_Adj_UCI_ecd), alpha = 0.4)\n",
        "    + facet_wrap(\"State\", scales = \"free\")\n",
        "    + theme_bw()\n",
        "    + labs(y = \"Total methane emissions (CH$_4$ kt year$^{-1}$)\", x = \"Year\")\n",
        "    + theme(panel_grid_major = element_blank(), panel_grid_minor = element_blank(), legend_position = \"bottom\")\n",
        "    + scale_x_continuous(limits = (1988, 2020))\n",
        ")\n",
        "\n",
        "save_plot(TotEmissions, filename = \"Results/Plot/TotEmissions_State.pdf\", width = 5.9, height = 5)\n",
        "save_plot(TotEmissions, filename = \"Results/Plot/TotEmissions_State.jpeg\", width = 5.9, height = 5)\n",
        "\n",
        "\n",
        "# TotEmissions2 (On the same graph as different lines)\n",
        "TotEmissions2 = (\n",
        "    ggplot(subset(Data6.kha.hist, State != \"Australia\"), aes(x = pd.to_numeric(FinYear) + 1, y = TotE_T3_kt_year_Adj_ecd))\n",
        "    + geom_line(aes(color = \"State\"), size = 1.5)\n",
        "    + theme_bw()\n",
        "    + theme(panel_grid_major = element_blank(), panel_grid_minor = element_blank(), legend_position = \"bottom\")\n",
        "    + labs(y = \"Total methane emissions (CH$_4$ kt year$^{-1}$)\", x = \"Year\", color = \"State or Territory\")\n",
        "    + scale_x_continuous(limits = (1988, 2020))\n",
        ")\n",
        "\n",
        "save_plot(TotEmissions2, filename = \"Results/Plot/TotEmissions_State2.pdf\", width = 10.9, height = 8)\n",
        "save_plot(TotEmissions2, filename = \"Results/Plot/TotEmissions_State2.jpeg\", width = 10.9, height = 8)\n",
        "\n",
        "\n",
        "# TotEmissions_both\n",
        "TotEmissions_both = (\n",
        "    TotEmissions_Aus\n",
        "    + TotEmissions2\n",
        "    + plot_layout(ncols = 1, align = \"hv\", labels = [\"A\", \"B\"], hjust = -5, vjust = 2)\n",
        ")\n",
        "\n",
        "save_plot(TotEmissions_both, filename = \"TotEmissions_both.pdf\", base_width = 6.2, base_height = 8.2)\n",
        "save_plot(TotEmissions_both, filename = \"TotEmissions_both.jpeg\", base_width = 6.2, base_height = 8.2)\n",
        "\n",
        "\n",
        "# Comparison of total methane emissions across methods\n",
        "Tier1 = Data6.kha.hist_Aus[Data6.kha.hist_Aus[\"FinYear\"] == 2020].copy()\n",
        "Tier1[\"Mean\"] = 0.183 * Tier1[\"SumArea_kha_Adj_ecd\"]\n",
        "Tier1[\"LCI\"] = 0.118 * Tier1[\"SumArea_kha_Adj_LCI_ecd\"]\n",
        "Tier1[\"UCI\"] = 0.228 * Tier1[\"SumArea_kha_Adj_UCI_ecd\"]\n",
        "Tier1[\"Method\"] = \"Tier 1\"\n",
        "\n",
        "Tier3 = Data6.kha.hist_Aus[Data6.kha.hist_Aus[\"FinYear\"] == 2020].copy()\n",
        "Tier3.rename(columns={\"TotE_T3_kt_year_Adj_ecd\": \"Mean\", \"TotE_T3_kt_year_Adj_LCI_ecd\": \"LCI\", \"TotE_T3_kt_year_Adj_UCI_ecd\": \"UCI\"}, inplace=True)\n",
        "Tier3[\"Method\"] = \"Tier 3\"\n",
        "\n",
        "# Approach for Tier2 (based on nls regression above)(search \"nls\")\n",
        "# For the LCI, I will multiply the predicted log-10 value by 0.8432157\n",
        "# For the UCI, I will multiply the predicted log-10 value by 1.100405\n",
        "Tier2 = Data5.kha[(Data5.kha[\"FinYear\"] == 2020) & (Data5.kha[\"State\"] == \"Australia\")].copy()\n",
        "Tier2[\"Mean\"] = Tier2[\"TotE_T2_kt_year_Adj\"].mean()\n",
        "Tier2[\"LCI\"] = 10 ** (0.8432157 * np.log10(Tier2[\"TotE_T2_kt_year_Adj\"]))\n",
        "Tier2[\"UCI\"] = 10 ** (1.100405 * np.log10(Tier2[\"TotE_T2_kt_year_Adj\"]))\n",
        "Tier2[\"Method\"] = \"Tier 2\"\n",
        "\n",
        "# Create the final dataset\n",
        "AllTiers = pd.concat([Tier1, Tier2, Tier3], ignore_index=True)\n",
        "\n",
        "AllTiers.to_csv(\"Results/Data/AllTiers.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "203043bb",
      "metadata": {
        "id": "203043bb"
      },
      "source": [
        "SECTION 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5468d134",
      "metadata": {
        "id": "5468d134"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e474285",
      "metadata": {
        "id": "7e474285"
      },
      "outputs": [],
      "source": [
        "# 9) ALL STATISTICS\n",
        "# ===========\n",
        "\n",
        "if False:\n",
        "\n",
        "    # 4.1 Historical increase in agricultural ponds\n",
        "    # ---\n",
        "\n",
        "    # Australia average increase\n",
        "    Incr1988to2020.Aus = Incr1988to2020[['Counts_1988', 'Counts_2020', 'SA_1988', 'SA_2020']].sum()\n",
        "    Incr1988to2020.Aus['Rel_Count_1988to2020'] = (Incr1988to2020.Aus['Counts_2020'] - Incr1988to2020.Aus['Counts_1988']) / Incr1988to2020.Aus['Counts_1988']\n",
        "    Incr1988to2020.Aus['Rel_SA_1988to2020'] = (Incr1988to2020.Aus['SA_2020'] - Incr1988to2020.Aus['SA_1988']) / Incr1988to2020.Aus['SA_1988']\n",
        "\n",
        "    # State-by-State relative increases\n",
        "    Incr1988to2020.sort_values(by='Rel_Count_1988to2020', ascending=False)\n",
        "\n",
        "\n",
        "    # 4.2 Water surface area of agricultural ponds\n",
        "    # ---\n",
        "\n",
        "    # Plot year-to-year variability\n",
        "    SA_YearToYear = ggplot(data=Data6.kha.hist, aes(x='FinYear', y='SumArea.kha.Adj')) +\n",
        "                    geom_line(aes(color='State'))\n",
        "\n",
        "    # Relative magnitude of year-to-year variability\n",
        "    SA_YearToYear_Rel = (Data6.kha.hist.Aus['SumArea.kha.Adj'].diff() / Data6.kha.hist.Aus['SumArea.kha.Adj'].shift(1)).mean()\n",
        "    SA_YearToYear_Rel = SA_YearToYear_Rel.dropna()\n",
        "    100 * SA_YearToYear_Rel.abs().mean()\n",
        "    SA_YearToYear_Rel.abs().max()\n",
        "    SA_YearToYear_Rel.abs().sort_values()\n",
        "\n",
        "    # Correlation with BOM rainfall anomalies\n",
        "    corr_test_result = Data6.kha.hist.Aus[['RelCapacityBOM.aus.FinYear', 'RelCapacityBOM.aus.Anomaly']].dropna().corr(method='pearson')\n",
        "    corr_test_result.loc['RelSurfaceArea', 'Anomaly']\n",
        "\n",
        "    # Consequences of climate conditions on year-to-year surface area\n",
        "    StatsAusSA = Data6.kha.hist.Aus[['FinYear', 'SumArea.kha.Adj.ecd']].dropna().diff()\n",
        "    StatsAusSA['AbsDifSA'] = StatsAusSA['SumArea.kha.Adj.ecd'].abs()\n",
        "    StatsAusSA['AbsDifSA'].describe()\n",
        "\n",
        "    # Compare surface area and relative capacity between 2016 and 2018/2019\n",
        "    Data6.kha.hist.Aus[['FinYear', 'SumArea.kha.Adj.ecd']].query('FinYear in [2016, 2018, 2019]')\n",
        "\n",
        "\n",
        "    # 4.3 Methane flux from agricultural ponds\n",
        "    # ---\n",
        "\n",
        "\n",
        "    # 4.4 Total methane emissions from agricultural ponds in Australia\n",
        "    # ---\n",
        "\n",
        "    # Range and max\n",
        "    Data6.kha.hist.Aus['TotE_T3.kt.year.Adj.ecd'].dropna().describe()\n",
        "\n",
        "    # Trend\n",
        "    Trend_Aus = np.polyfit(Data6.kha.hist.Aus['FinYear'], Data6.kha.hist.Aus['TotE_T3.kt.year.Adj.ecd'], 1)\n",
        "    Trend_Aus = pd.Series(Trend_Aus, index=['Slope', 'Intercept'])\n",
        "    Trend_Aus\n",
        "\n",
        "    # Year-to-year variability\n",
        "    year_to_year_variability = 100 * (0.07309 / 0.6438)\n",
        "\n",
        "    # Is this increase due to stronger fluxes? No\n",
        "    Trend_Aus_onlyClimate = np.polyfit(Data6.kha.hist.Aus['FinYear'], Data6.kha.hist.Aus['TotE_T3.kt.year.Adj'], 1)\n",
        "    Trend_Aus_onlyClimate = pd.Series(Trend_Aus_onlyClimate, index=['Slope', 'Intercept'])\n",
        "    Trend_Aus_onlyClimate\n",
        "\n",
        "    # Is this increase due to larger dams? No\n",
        "    Trend_Aus_onlySA = np.polyfit(Data6.kha.hist.Aus['FinYear'], Data6.kha.hist.Aus['SumArea.kha.Adj'], 1)\n",
        "    Trend_Aus_onlySA = pd.Series(Trend_Aus_onlySA, index=['Slope', 'Intercept'])\n",
        "    Trend_Aus_onlySA\n",
        "\n",
        "    # Is this increase due to more dams being created? Yes\n",
        "    Trend_Aus_num = np.polyfit(Data6.kha.hist.Aus['FinYear'], Data6.kha.hist.Aus['SumCount.Adj.ecd'], 1)\n",
        "    Trend_Aus_num = pd.Series(Trend_Aus_num, index=['Slope', 'Intercept'])\n",
        "    Trend_Aus_num\n",
        "\n",
        "    # What is the effect on surface water?\n",
        "    Trend_Aus_sw = np.polyfit(Data6.kha.hist.Aus['FinYear'], Data6.kha.hist.Aus['SumArea.kha.Adj.ecd'], 1)\n",
        "    Trend_Aus_sw = pd.Series(Trend_Aus_sw, index=['Slope', 'Intercept'])\n",
        "    Trend_Aus_sw\n",
        "\n",
        "\n",
        "    # State by state\n",
        "    subset(Data6.kha.hist, State == \"New South Wales\" & FinYear == 2020)['TotE_T3.kt.year.Adj.ecd']\n",
        "    subset(Data6.kha.hist, State == \"Queensland\" & FinYear == 2020)['TotE_T3.kt.year.Adj.ecd']\n",
        "    subset(Data6.kha.hist, State == \"Victoria\" & FinYear == 2020)['TotE_T3.kt.year.Adj.ecd']\n",
        "    subset(Data6.kha.hist, State == \"Western Australia\" & FinYear == 2020)['TotE_T3.kt.year.Adj.ecd']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "065121d5",
      "metadata": {
        "id": "065121d5"
      },
      "source": [
        "SECTION 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55045555",
      "metadata": {
        "id": "55045555"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36242f1a",
      "metadata": {
        "id": "36242f1a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "\n",
        "# ------\n",
        "# Average total methane emission at 15C\n",
        "# ------\n",
        "\n",
        "# Load the average farm dam emissions at 15C in mg.m2.day\n",
        "# Assume the data is loaded from TotMet_15C.lm.RData in R\n",
        "FarmDams_CH4kgyearha_at15C = 365.25 * 10000 * (10 ** fixef(TotMet_15C.lm)) / 1e+6\n",
        "FarmDams_CH4kgyearha_at15C_LCI = 365.25 * 10000 * (10 ** confint(TotMet_15C.lm)[3, 0]) / 1e+6\n",
        "FarmDams_CH4kgyearha_at15C_UCI = 365.25 * 10000 * (10 ** confint(TotMet_15C.lm)[3, 1]) / 1e+6\n",
        "\n",
        "# CV for the relative error\n",
        "Emissions_LCI = (FarmDams_CH4kgyearha_at15C - FarmDams_CH4kgyearha_at15C_LCI) / FarmDams_CH4kgyearha_at15C\n",
        "Emissions_UCI = (FarmDams_CH4kgyearha_at15C_UCI - FarmDams_CH4kgyearha_at15C) / FarmDams_CH4kgyearha_at15C\n",
        "Emissions_CV = np.mean(np.concatenate((Emissions_LCI, Emissions_UCI)))\n",
        "\n",
        "# -------\n",
        "# Farm dam area\n",
        "# -------\n",
        "if False:\n",
        "    # Assume AreaVal.csv is read into a DataFrame named AreaVal\n",
        "    AreaVal_lm = sm.OLS(AreaVal['Obs'], sm.add_constant(AreaVal['Pred'])).fit()\n",
        "    AreaVal_lm_summary = AreaVal_lm.summary()\n",
        "\n",
        "    # Calculate correlation\n",
        "    correlation = AreaVal['Obs'].corr(AreaVal['Pred'])\n",
        "\n",
        "    # MAPE: Mean Absolute Percentage Error\n",
        "    mape = np.mean(np.abs((AreaVal['Obs'] - AreaVal['Pred']) / AreaVal['Obs'])) * 100\n",
        "\n",
        "    # MAE: Mean Absolute Error\n",
        "    mae_val = mae(AreaVal['Obs'], AreaVal['Pred']) / np.mean(AreaVal['Obs'])\n",
        "\n",
        "    # Plot and save\n",
        "    plt.figure()\n",
        "    sns.scatterplot(x='Pred', y='Obs', data=AreaVal)\n",
        "    sns.regplot(x='Pred', y='Obs', data=AreaVal, scatter=False, line_kws={\"color\": \"red\"})\n",
        "    plt.annotate(\"Mean Absolute Perc. Error: 32.4%\", xy=(3000, 16000), size=4.5)\n",
        "    plt.savefig(\"Results/Plot/Validation_weather_to_water.jpeg\")\n",
        "\n",
        "    plt.figure()\n",
        "    sns.scatterplot(x='Obs', y='Pred', data=AreaVal[AreaVal['Obs'] > 0])\n",
        "    plt.xscale('log')\n",
        "    plt.yscale('log')\n",
        "    plt.savefig(\"Results/Plot/Validation_weather_to_water_log.jpeg\")\n",
        "\n",
        "# What about the error of measuring farm dams using satellite data?\n",
        "if False:\n",
        "    # Assume Together.csv is read into a DataFrame named AreaVal2\n",
        "    AreaVal2_lm = sm.OLS(AreaVal2[AreaVal2['Type'] == \"Measuring surface water with satellite data\"]['Obs'],\n",
        "                         sm.add_constant(AreaVal2[AreaVal2['Type'] == \"Measuring surface water with satellite data\"]['Pred'])).fit()\n",
        "\n",
        "    mae_val = mae(AreaVal2[~np.isnan(AreaVal2['Obs'])]['Obs'],\n",
        "                  AreaVal2[~np.isnan(AreaVal2['Obs'])]['Pred']) / np.mean(AreaVal2[~np.isnan(AreaVal2['Obs'])]['Obs'])\n",
        "\n",
        "    # Plot and save\n",
        "    g = sns.FacetGrid(AreaVal2, col='Type', col_wrap=3, sharex=False, sharey=False)\n",
        "    g.map(sns.scatterplot, 'Obs', 'Pred')\n",
        "    g.map(sns.regplot, 'Obs', 'Pred', scatter=False, line_kws={\"color\": \"red\"})\n",
        "    g.savefig(\"Results/Plot/Validation_weather_to_water.jpeg\")\n",
        "\n",
        "# Decide on a CV\n",
        "FarmDamArea_CV = 0.324 + 0.112\n",
        "\n",
        "# -------\n",
        "# Temp sensitivity\n",
        "# -------\n",
        "# Assume Boltzmann.lm.RData is loaded into a variable named TempSensitivity///\n",
        "#YOU MUST LOAD THE BOLTZMAN.LM DATA/CHECK THE R SCRIPT FOR REFERENCE\n",
        "TempEmissions = {'mean': TempSensitivity.params[1],\n",
        "                 'LCI': confint(TempSensitivity)[1][0],\n",
        "                 'UCI': confint(TempSensitivity)[1][1]}\n",
        "TempEmissions_CV_LCI = (TempEmissions['mean'] - TempEmissions['LCI']) / TempEmissions['mean']\n",
        "TempEmissions_CV_UCI = (TempEmissions['UCI'] - TempEmissions['mean']) / TempEmissions['mean']\n",
        "TempEmissions_CV = np.mean([TempEmissions_CV_LCI, TempEmissions_CV_UCI])\n",
        "\n",
        "# All CVs\n",
        "CV = {'TempSensitivity': TempEmissions_CV, 'Area': FarmDamArea_CV, 'MeanEmissions': Emissions_CV}\n",
        "labels = ['Temp. Sensitivity*\\n(df = 251)', 'Water Surface \\n(df = 12,235)', 'Methane Flux\\n(df = 286)']\n",
        "\n",
        "# Source of df:\n",
        "# MetEmissions: length(FarmDamData$CH4_mgm2day.EbuAdj.15C) 286 (FigS2)\n",
        "# TempSensitivity: length(RosData_nozeros$fch4_mgCH4m2d) 251 (FigS1)\n",
        "# RatioData.CV: length(RatioData.data$Ratio_D_to_E) 162 (FigS3)\n",
        "# Pond Area: weather-to-water model\n",
        "\n",
        "AllCV = pd.DataFrame({'labels': labels, 'CV': list(CV.values())})\n",
        "AllCV['CV.std'] = AllCV['CV'] / AllCV['CV'].sum()\n",
        "AllCV['labels'] = pd.Categorical(AllCV['labels'], categories=labels, ordered=True)\n",
        "AllCV.to_csv(\"Results/Data/AllCV.csv\", index=False)\n",
        "\n",
        "# Plot sources of uncertainty\n",
        "plt.figure()\n",
        "sns.barplot(x='labels', y='CV', data=AllCV)\n",
        "plt.ylabel(\"Coefficient of variation of the mean\")\n",
        "plt.ylim(0, 1)\n",
        "plt.savefig(\"Results/Plot/CV.pdf\")\n",
        "plt.savefig(\"Results/Plot/CV.jpeg\")\n",
        "\n",
        "# ----\n",
        "# Statistics:\n",
        "# ----\n",
        "# All CVs\n",
        "print(AllCV)\n",
        "\n",
        "# Repercussions for\n",
        "# Assume Data6.kha.hist.Aus is loaded into a DataFrame\n",
        "print(Data6.kha.hist.Aus['TotE_T3.kt.year.Adj.ecd'])\n",
        "print(Data6.kha.hist.Aus['TotE_T3.kt.year.Adj.LCI.ecd'])\n",
        "print(Data6.kha.hist.Aus['TotE_T3.kt.year.Adj.UCI.ecd'])\n",
        "\n",
        "# Fold difference in predictions\n",
        "print(Data6.kha.hist.Aus['TotE_T3.kt.year.Adj.UCI.ecd'] / Data6.kha.hist.Aus['TotE_T3.kt.year.Adj.ecd'])\n",
        "print(Data6.kha.hist.Aus['TotE_T3.kt.year.Adj.ecd'] / Data6.kha.hist.Aus['TotE_T3.kt.year.Adj.LCI.ecd'])\n",
        "\n",
        "print(np.nanmin(np.array([Data6.kha.hist.Aus['TotE_T3.kt.year.Adj.UCI.ecd'] / Data6.kha.hist.Aus['TotE_T3.kt.year.Adj.ecd'],\n",
        "                           Data6.kha.hist.Aus['TotE_T3.kt.year.Adj.ecd'] / Data6.kha.hist.Aus['TotE_T3.kt.year.Adj.LCI.ecd']])))\n",
        "\n",
        "# Load AllCV from CSV\n",
        "AllCV = pd.read_csv(\"Results/Data/AllCV.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
